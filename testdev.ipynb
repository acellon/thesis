{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test/Dev Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import chb\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "import lasagne.objectives as lobj\n",
    "import lasagne.nonlinearities as lnon\n",
    "import lasagne.layers as llay\n",
    "import lasagne.init as linit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Subject Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: /Users/adamcellon/Drive/senior/thesis/data/chb01.npz\n",
      "Done: 68.598263 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# TODO: organize this differently - this should just load the subject and\n",
    "#       data, so that we can loop through all the LOOs in main\n",
    "# Load data for subject\n",
    "subject = chb.CHBsubj()\n",
    "subject.load_meta('chb01')\n",
    "subject.load_data(exthd=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load and read training and test set images and labels.\n",
    "x_train, y_train, x_test, y_test = subject.leaveOneOut(1, 1000, 100)\n",
    "\n",
    "# We reserve the last 100 training examples for validation.\n",
    "x_train, x_val = x_train[:-500], x_train[-500:]\n",
    "y_train, y_val = y_train[:-500], y_train[-500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet Based On [This Site](https://github.com/luizgh/lasagne_basics/blob/master/comparing-optimization-algs.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_size=(None,1,23,256) # Batch size x Img Channels x Height (electrodes) x Width (samples)\n",
    "output_size=1             # Binary classification of seizure (1) vs non-seizure (0)\n",
    "\n",
    "def build_model():\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "    net = {}\n",
    "\n",
    "    #Input layer:\n",
    "    net['data'] = lasagne.layers.InputLayer(data_size, input_var=input_var)\n",
    "\n",
    "    #Convolution + Pooling\n",
    "    net['conv1'] = lasagne.layers.Conv2DLayer(net['data'], num_filters=6, filter_size=(1,16), stride=(1,2), nonlinearity=lnon.sigmoid)\n",
    "    net['pool1'] = lasagne.layers.Pool2DLayer(net['conv1'], pool_size=(1,2))\n",
    "\n",
    "    net['conv2'] = lasagne.layers.Conv2DLayer(net['pool1'], num_filters=10, filter_size=(1,32), stride=(1,2), nonlinearity=lnon.sigmoid)\n",
    "    net['pool2'] = lasagne.layers.Pool2DLayer(net['conv2'], pool_size=(1,2))\n",
    "\n",
    "\n",
    "    #Fully-connected + dropout\n",
    "    net['fc1'] = lasagne.layers.DenseLayer(net['pool2'], num_units=100)\n",
    "\n",
    "    #Output layer:\n",
    "    net['out'] = lasagne.layers.DenseLayer(net['fc1'], num_units=output_size, \n",
    "                                           nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    weight_decay = 1e-5\n",
    "\n",
    "    #Loss function: mean cross-entropy\n",
    "    prediction = lasagne.layers.get_output(net['out'])\n",
    "    loss = lasagne.objectives.binary_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    #Also add weight decay to the cost function\n",
    "    weightsl2 = lasagne.regularization.regularize_network_params(net['out'], lasagne.regularization.l2)\n",
    "    loss += weight_decay * weightsl2\n",
    "\n",
    "    #Get the update rule\n",
    "    params = lasagne.layers.get_all_params(net['out'], trainable=True)\n",
    "    updates = lasagne.updates.sgd(loss, params, learning_rate=1e-2)\n",
    "\n",
    "    test_prediction = lasagne.layers.get_output(net['out'], deterministic=True)\n",
    "    test_loss = lasagne.objectives.binary_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, name='train')\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc], name='validation')\n",
    "    get_preds = theano.function([input_var], test_prediction, name='get_preds')\n",
    "\n",
    "    return (train_fn, val_fn, get_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size=10\n",
    "\n",
    "#Run the training function per mini-batches\n",
    "n_examples = x_train.shape[0]\n",
    "n_batches = int(n_examples / batch_size)\n",
    "\n",
    "def train(train_fn):\n",
    "    cost_history = []\n",
    "    for epoch in range(epochs):\n",
    "        st = time.time()\n",
    "        batch_cost_history = []\n",
    "        for batch in range(n_batches):\n",
    "            x_batch = x_train[batch*batch_size: (batch+1) * batch_size]\n",
    "            y_batch = y_train[batch*batch_size: (batch+1) * batch_size]\n",
    "\n",
    "            this_cost = train_fn(x_batch, y_batch) # This is where the model gets updated\n",
    "\n",
    "            batch_cost_history.append(this_cost)\n",
    "        epoch_cost = np.mean(batch_cost_history)\n",
    "        cost_history.append(epoch_cost)\n",
    "        en = time.time()\n",
    "        print('Epoch %d/%d, train error: %f. Elapsed time: %.2f seconds' % (epoch+1, epochs, epoch_cost, en-st))\n",
    "    return cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD\n",
      "Epoch 1/10, train error: nan. Elapsed time: 9.71 seconds\n",
      "Epoch 2/10, train error: nan. Elapsed time: 9.26 seconds\n",
      "Epoch 3/10, train error: nan. Elapsed time: 10.07 seconds\n",
      "Epoch 4/10, train error: nan. Elapsed time: 9.46 seconds\n",
      "Epoch 5/10, train error: nan. Elapsed time: 9.43 seconds\n",
      "Epoch 6/10, train error: nan. Elapsed time: 10.35 seconds\n",
      "Epoch 7/10, train error: nan. Elapsed time: 10.35 seconds\n",
      "Epoch 8/10, train error: nan. Elapsed time: 13.33 seconds\n",
      "Epoch 9/10, train error: nan. Elapsed time: 11.77 seconds\n",
      "Epoch 10/10, train error: nan. Elapsed time: 9.93 seconds\n"
     ]
    }
   ],
   "source": [
    "sgd_functions = build_model()\n",
    "for key, value in network.items():\n",
    "    print('%s: %s' % (key, llay.get_output_shape(value)))\n",
    "print (\"Training with SGD\")\n",
    "sgd_cost_history = train(sgd_functions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet Based On mnist.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ##################### Build the neural network model #######################\n",
    "def build_cnn(input_var=None):\n",
    "    data_size = (None,1,23,256)\n",
    "    output_size = 1\n",
    "    # The most simple, straightforward CNN I can come up with.\n",
    "    network = {}\n",
    "    \n",
    "    network['data'] = llay.InputLayer(shape=data_size, input_var=input_var)\n",
    "\n",
    "    network['conv1'] = llay.Conv2DLayer(network['data'], num_filters=8, filter_size=(1, 15),\n",
    "                                        stride=(1,1), nonlinearity=lnon.sigmoid,pad='same')\n",
    "\n",
    "    network['pool1'] = llay.MaxPool2DLayer(network['conv1'], pool_size=(1,2))\n",
    "\n",
    "    network['conv2'] = llay.Conv2DLayer(network['pool1'], num_filters=8, filter_size=(1, 15), \n",
    "                                        stride=(1,2), nonlinearity=lnon.rectify)\n",
    "    \n",
    "    network['dense1'] = llay.DenseLayer(network['conv2'],num_units=256, nonlinearity=lnon.rectify)\n",
    "\n",
    "    network['out'] = llay.DenseLayer(network['dense1'], num_units=output_size, nonlinearity=lnon.rectify)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "# Notice that this function returns only mini-batches of size `batchsize`.\n",
    "# If the size of the data is not a multiple of `batchsize`, it will not\n",
    "# return the last (remaining) mini-batch.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n"
     ]
    }
   ],
   "source": [
    "# Create neural network model (depending on first command line parameter)\n",
    "print(\"Building model and compiling functions...\")\n",
    "network = build_cnn(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a loss expression for training, i.e., a scalar objective we want\n",
    "# to minimize (for our 2-class problem, it is the cross-entropy loss):\n",
    "prediction = llay.get_output(network['out'])\n",
    "\n",
    "loss = lobj.binary_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "\n",
    "# Update expressions for training\n",
    "params = llay.get_all_params(network['out'], trainable=True)\n",
    "updates = lasagne.updates.rmsprop(loss, params, learning_rate=0.001)\n",
    "\n",
    "# Create a loss expression for validation/testing.\n",
    "test_prediction = llay.get_output(network['out'], deterministic=True)\n",
    "\n",
    "test_loss = lobj.binary_crossentropy(test_prediction,target_var)\n",
    "test_loss = lobj.aggregate(test_loss, mode='mean')\n",
    "\n",
    "test_acc = T.mean(lobj.binary_accuracy(test_prediction, target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: (None, 1, 23, 256)\n",
      "conv1: (None, 8, 23, 256)\n",
      "pool1: (None, 8, 23, 128)\n",
      "conv2: (None, 8, 23, 57)\n",
      "dense1: (None, 256)\n",
      "out: (None, 1)\n"
     ]
    }
   ],
   "source": [
    "for key, value in network.items():\n",
    "    print('%s: %s' % (key, llay.get_output_shape(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1 of 10 took 2.877s\n",
      "  training error:\t\tnan\n",
      "  train batches:\t\t38\n",
      "  training loss:\t\tnan\n",
      "  validation error:\t\tnan\n",
      "  validation batches:\t\t38\n",
      "  validation loss:\t\tnan\n",
      "  validation accuracy:\t\t62.55 %\n",
      "Epoch 2 of 10 took 2.683s\n",
      "  training error:\t\tnan\n",
      "  train batches:\t\t38\n",
      "  training loss:\t\tnan\n",
      "  validation error:\t\tnan\n",
      "  validation batches:\t\t38\n",
      "  validation loss:\t\tnan\n",
      "  validation accuracy:\t\t62.55 %\n",
      "Epoch 3 of 10 took 2.685s\n",
      "  training error:\t\tnan\n",
      "  train batches:\t\t38\n",
      "  training loss:\t\tnan\n",
      "  validation error:\t\tnan\n",
      "  validation batches:\t\t38\n",
      "  validation loss:\t\tnan\n",
      "  validation accuracy:\t\t62.55 %\n",
      "Epoch 4 of 10 took 2.694s\n",
      "  training error:\t\tnan\n",
      "  train batches:\t\t38\n",
      "  training loss:\t\tnan\n",
      "  validation error:\t\tnan\n",
      "  validation batches:\t\t38\n",
      "  validation loss:\t\tnan\n",
      "  validation accuracy:\t\t62.55 %\n",
      "Epoch 5 of 10 took 2.687s\n",
      "  training error:\t\tnan\n",
      "  train batches:\t\t38\n",
      "  training loss:\t\tnan\n",
      "  validation error:\t\tnan\n",
      "  validation batches:\t\t38\n",
      "  validation loss:\t\tnan\n",
      "  validation accuracy:\t\t62.55 %\n",
      "Epoch 6 of 10 took 2.686s\n",
      "  training error:\t\tnan\n",
      "  train batches:\t\t38\n",
      "  training loss:\t\tnan\n",
      "  validation error:\t\tnan\n",
      "  validation batches:\t\t38\n",
      "  validation loss:\t\tnan\n",
      "  validation accuracy:\t\t62.55 %\n",
      "Epoch 7 of 10 took 2.689s\n",
      "  training error:\t\tnan\n",
      "  train batches:\t\t38\n",
      "  training loss:\t\tnan\n",
      "  validation error:\t\tnan\n",
      "  validation batches:\t\t38\n",
      "  validation loss:\t\tnan\n",
      "  validation accuracy:\t\t62.55 %\n",
      "Epoch 8 of 10 took 3.329s\n",
      "  training error:\t\tnan\n",
      "  train batches:\t\t38\n",
      "  training loss:\t\tnan\n",
      "  validation error:\t\tnan\n",
      "  validation batches:\t\t38\n",
      "  validation loss:\t\tnan\n",
      "  validation accuracy:\t\t62.55 %\n",
      "Epoch 9 of 10 took 3.202s\n",
      "  training error:\t\tnan\n",
      "  train batches:\t\t38\n",
      "  training loss:\t\tnan\n",
      "  validation error:\t\tnan\n",
      "  validation batches:\t\t38\n",
      "  validation loss:\t\tnan\n",
      "  validation accuracy:\t\t62.55 %\n",
      "Epoch 10 of 10 took 3.143s\n",
      "  training error:\t\tnan\n",
      "  train batches:\t\t38\n",
      "  training loss:\t\tnan\n",
      "  validation error:\t\tnan\n",
      "  validation batches:\t\t38\n",
      "  validation loss:\t\tnan\n",
      "  validation accuracy:\t\t62.55 %\n"
     ]
    }
   ],
   "source": [
    "num_epochs=10\n",
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(x_train, y_train, 10):\n",
    "        inputs, targets = batch\n",
    "        #print(inputs.shape, targets.shape)\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(x_val, y_val, 10):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training error:\\t\\t{:.6f}\".format(train_err)) #\n",
    "    print(\"  train batches:\\t\\t{:d}\".format(train_batches)) #\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation error:\\t\\t{:.6f}\".format(val_err)) #\n",
    "    print(\"  validation batches:\\t\\t{:d}\".format(val_batches)) #\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test loss:\t\t\tnan\n",
      "  test accuracy:\t\t59.00 %\n"
     ]
    }
   ],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(x_test, y_test, 2, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
