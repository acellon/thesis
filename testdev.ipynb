{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First attempt at Lasagne with my real data.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import chb\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "import lasagne.objectives as lobj\n",
    "import lasagne.nonlinearities as lnon\n",
    "import lasagne.layers as llay\n",
    "import lasagne.init as linit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ################## Download and prepare the CHBMIT dataset ##################\n",
    "# Loads data for a certain subject (taken as a string 'chbXX').\n",
    "\n",
    "def load_dataset(subjname):\n",
    "    # TODO: organize this differently - this should just load the subject and\n",
    "    #       data, so that we can loop through all the LOOs in main\n",
    "    # Load data for subject\n",
    "    subject = chb.CHBsubj()\n",
    "    subject.load_meta(subjname)\n",
    "    subject.load_data(exthd=False)\n",
    "\n",
    "    # Load and read training and test set images and labels.\n",
    "    x_train, y_train, x_test, y_test = subject.leaveOneOut(1)\n",
    "\n",
    "    # We reserve the last 100 training examples for validation.\n",
    "    x_train, x_val = x_train[:-100], x_train[-100:]\n",
    "    y_train, y_val = y_train[:-100], y_train[-100:]\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##################### Build the neural network model #######################\n",
    "def build_cnn(input_var=None):\n",
    "    # The most simple, straightforward CNN I can come up with.\n",
    "    \n",
    "    # Input layer, as usual:\n",
    "    l_in = llay.InputLayer(shape=(None, 1, 23, 256),\n",
    "                                        input_var=input_var)\n",
    "\n",
    "    # Convolutional layer with 32 kernels of size 1x32. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    l_hid1 = llay.Conv2DLayer(l_in, num_filters=16, filter_size=(1, 32),\n",
    "                                        stride=(1,8), nonlinearity=lnon.sigmoid,\n",
    "                                        W=linit.GlorotUniform())\n",
    "\n",
    "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
    "    '''network = llay.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(1, 16),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = llay.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    '''\n",
    "    # A fully-connected layer of 256 units with 25% dropout on its inputs:\n",
    "    l_hid2 = llay.DenseLayer(\n",
    "            llay.dropout(l_hid1, p=.25),\n",
    "            num_units=256,\n",
    "            nonlinearity=lnon.sigmoid)\n",
    "\n",
    "    # And, finally, the 2-unit output layer with 25% dropout on its inputs:\n",
    "    out = llay.DenseLayer(\n",
    "            llay.dropout(l_hid2, p=.25),\n",
    "            num_units=2,\n",
    "            nonlinearity=lnon.sigmoid)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "# Notice that this function returns only mini-batches of size `batchsize`.\n",
    "# If the size of the data is not a multiple of `batchsize`, it will not\n",
    "# return the last (remaining) mini-batch.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: /Users/adamcellon/Drive/senior/thesis/data/chb01.npz\n",
      "Done: 76.592338 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "subject='chb01'\n",
    "# Load the dataset\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = load_dataset(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n"
     ]
    }
   ],
   "source": [
    "# Create neural network model (depending on first command line parameter)\n",
    "print(\"Building model and compiling functions...\")\n",
    "network = build_cnn(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a loss expression for training, i.e., a scalar objective we want\n",
    "# to minimize (for our 2-class problem, it is the cross-entropy loss):\n",
    "prediction = llay.get_output(network)\n",
    "\n",
    "loss = lobj.binary_crossentropy(prediction, target_var)\n",
    "loss = lobj.aggregate(loss, mode='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD).\n",
    "\n",
    "params = llay.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.sgd(loss, params, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "\n",
    "test_prediction = llay.get_output(network, deterministic=True)\n",
    "test_loss = lobj.binary_crossentropy(test_prediction,target_var)\n",
    "test_loss = lobj.aggregate(test_loss, mode='mean')\n",
    "\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(lobj.binary_accuracy(test_prediction, target_var),\n",
    "                  dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1 of 50 took 2.202s\n",
      "  training loss:\t\t0.746386\n",
      "  validation loss:\t\t0.671676\n",
      "  validation accuracy:\t\t61.00 %\n",
      "Epoch 2 of 50 took 2.117s\n",
      "  training loss:\t\t0.702222\n",
      "  validation loss:\t\t0.651293\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 3 of 50 took 2.198s\n",
      "  training loss:\t\t0.673848\n",
      "  validation loss:\t\t0.641206\n",
      "  validation accuracy:\t\t63.50 %\n",
      "Epoch 4 of 50 took 2.174s\n",
      "  training loss:\t\t0.653969\n",
      "  validation loss:\t\t0.653770\n",
      "  validation accuracy:\t\t61.00 %\n",
      "Epoch 5 of 50 took 2.146s\n",
      "  training loss:\t\t0.642635\n",
      "  validation loss:\t\t0.627299\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 6 of 50 took 2.208s\n",
      "  training loss:\t\t0.632754\n",
      "  validation loss:\t\t0.654959\n",
      "  validation accuracy:\t\t64.00 %\n",
      "Epoch 7 of 50 took 2.213s\n",
      "  training loss:\t\t0.622264\n",
      "  validation loss:\t\t0.634043\n",
      "  validation accuracy:\t\t66.50 %\n",
      "Epoch 8 of 50 took 2.018s\n",
      "  training loss:\t\t0.620116\n",
      "  validation loss:\t\t0.613205\n",
      "  validation accuracy:\t\t66.50 %\n",
      "Epoch 9 of 50 took 2.150s\n",
      "  training loss:\t\t0.610566\n",
      "  validation loss:\t\t0.633147\n",
      "  validation accuracy:\t\t68.00 %\n",
      "Epoch 10 of 50 took 2.163s\n",
      "  training loss:\t\t0.618364\n",
      "  validation loss:\t\t0.615947\n",
      "  validation accuracy:\t\t69.00 %\n",
      "Epoch 11 of 50 took 2.013s\n",
      "  training loss:\t\t0.619732\n",
      "  validation loss:\t\t0.605633\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 12 of 50 took 2.019s\n",
      "  training loss:\t\t0.614024\n",
      "  validation loss:\t\t0.613938\n",
      "  validation accuracy:\t\t68.50 %\n",
      "Epoch 13 of 50 took 2.202s\n",
      "  training loss:\t\t0.597119\n",
      "  validation loss:\t\t0.608972\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 14 of 50 took 2.227s\n",
      "  training loss:\t\t0.606201\n",
      "  validation loss:\t\t0.615639\n",
      "  validation accuracy:\t\t68.50 %\n",
      "Epoch 15 of 50 took 2.028s\n",
      "  training loss:\t\t0.604890\n",
      "  validation loss:\t\t0.604897\n",
      "  validation accuracy:\t\t70.50 %\n",
      "Epoch 16 of 50 took 2.119s\n",
      "  training loss:\t\t0.602961\n",
      "  validation loss:\t\t0.641475\n",
      "  validation accuracy:\t\t64.50 %\n",
      "Epoch 17 of 50 took 2.021s\n",
      "  training loss:\t\t0.596763\n",
      "  validation loss:\t\t0.602871\n",
      "  validation accuracy:\t\t71.00 %\n",
      "Epoch 18 of 50 took 2.074s\n",
      "  training loss:\t\t0.580844\n",
      "  validation loss:\t\t0.607481\n",
      "  validation accuracy:\t\t69.50 %\n",
      "Epoch 19 of 50 took 2.203s\n",
      "  training loss:\t\t0.592721\n",
      "  validation loss:\t\t0.605527\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 20 of 50 took 2.207s\n",
      "  training loss:\t\t0.597983\n",
      "  validation loss:\t\t0.601748\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 21 of 50 took 2.008s\n",
      "  training loss:\t\t0.565476\n",
      "  validation loss:\t\t0.606745\n",
      "  validation accuracy:\t\t69.50 %\n",
      "Epoch 22 of 50 took 2.019s\n",
      "  training loss:\t\t0.594957\n",
      "  validation loss:\t\t0.605320\n",
      "  validation accuracy:\t\t70.50 %\n",
      "Epoch 23 of 50 took 2.142s\n",
      "  training loss:\t\t0.588993\n",
      "  validation loss:\t\t0.601430\n",
      "  validation accuracy:\t\t69.00 %\n",
      "Epoch 24 of 50 took 2.169s\n",
      "  training loss:\t\t0.591575\n",
      "  validation loss:\t\t0.618623\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 25 of 50 took 2.058s\n",
      "  training loss:\t\t0.594456\n",
      "  validation loss:\t\t0.606502\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 26 of 50 took 2.200s\n",
      "  training loss:\t\t0.585554\n",
      "  validation loss:\t\t0.599866\n",
      "  validation accuracy:\t\t69.50 %\n",
      "Epoch 27 of 50 took 2.085s\n",
      "  training loss:\t\t0.586128\n",
      "  validation loss:\t\t0.602623\n",
      "  validation accuracy:\t\t69.50 %\n",
      "Epoch 28 of 50 took 2.068s\n",
      "  training loss:\t\t0.585646\n",
      "  validation loss:\t\t0.622712\n",
      "  validation accuracy:\t\t66.00 %\n",
      "Epoch 29 of 50 took 2.368s\n",
      "  training loss:\t\t0.567629\n",
      "  validation loss:\t\t0.627292\n",
      "  validation accuracy:\t\t65.50 %\n",
      "Epoch 30 of 50 took 2.045s\n",
      "  training loss:\t\t0.600623\n",
      "  validation loss:\t\t0.602927\n",
      "  validation accuracy:\t\t68.50 %\n",
      "Epoch 31 of 50 took 2.077s\n",
      "  training loss:\t\t0.572397\n",
      "  validation loss:\t\t0.607228\n",
      "  validation accuracy:\t\t68.50 %\n",
      "Epoch 32 of 50 took 2.557s\n",
      "  training loss:\t\t0.592640\n",
      "  validation loss:\t\t0.603772\n",
      "  validation accuracy:\t\t70.50 %\n",
      "Epoch 33 of 50 took 2.276s\n",
      "  training loss:\t\t0.593090\n",
      "  validation loss:\t\t0.607551\n",
      "  validation accuracy:\t\t69.00 %\n",
      "Epoch 34 of 50 took 2.089s\n",
      "  training loss:\t\t0.604027\n",
      "  validation loss:\t\t0.606725\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 35 of 50 took 2.068s\n",
      "  training loss:\t\t0.586195\n",
      "  validation loss:\t\t0.621187\n",
      "  validation accuracy:\t\t64.50 %\n",
      "Epoch 36 of 50 took 2.072s\n",
      "  training loss:\t\t0.584543\n",
      "  validation loss:\t\t0.637068\n",
      "  validation accuracy:\t\t67.00 %\n",
      "Epoch 37 of 50 took 2.105s\n",
      "  training loss:\t\t0.595941\n",
      "  validation loss:\t\t0.624115\n",
      "  validation accuracy:\t\t67.00 %\n",
      "Epoch 38 of 50 took 2.073s\n",
      "  training loss:\t\t0.582574\n",
      "  validation loss:\t\t0.609665\n",
      "  validation accuracy:\t\t69.00 %\n",
      "Epoch 39 of 50 took 2.522s\n",
      "  training loss:\t\t0.573391\n",
      "  validation loss:\t\t0.608567\n",
      "  validation accuracy:\t\t68.00 %\n",
      "Epoch 40 of 50 took 2.207s\n",
      "  training loss:\t\t0.581407\n",
      "  validation loss:\t\t0.612935\n",
      "  validation accuracy:\t\t69.00 %\n",
      "Epoch 41 of 50 took 2.116s\n",
      "  training loss:\t\t0.578003\n",
      "  validation loss:\t\t0.607799\n",
      "  validation accuracy:\t\t66.50 %\n",
      "Epoch 42 of 50 took 2.311s\n",
      "  training loss:\t\t0.567253\n",
      "  validation loss:\t\t0.606808\n",
      "  validation accuracy:\t\t68.50 %\n",
      "Epoch 43 of 50 took 2.191s\n",
      "  training loss:\t\t0.585561\n",
      "  validation loss:\t\t0.608812\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 44 of 50 took 2.270s\n",
      "  training loss:\t\t0.585385\n",
      "  validation loss:\t\t0.605804\n",
      "  validation accuracy:\t\t69.50 %\n",
      "Epoch 45 of 50 took 2.061s\n",
      "  training loss:\t\t0.568068\n",
      "  validation loss:\t\t0.623741\n",
      "  validation accuracy:\t\t69.00 %\n",
      "Epoch 46 of 50 took 2.081s\n",
      "  training loss:\t\t0.573072\n",
      "  validation loss:\t\t0.605678\n",
      "  validation accuracy:\t\t69.50 %\n",
      "Epoch 47 of 50 took 2.209s\n",
      "  training loss:\t\t0.552795\n",
      "  validation loss:\t\t0.605998\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 48 of 50 took 2.382s\n",
      "  training loss:\t\t0.562529\n",
      "  validation loss:\t\t0.600737\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 49 of 50 took 2.209s\n",
      "  training loss:\t\t0.584791\n",
      "  validation loss:\t\t0.605268\n",
      "  validation accuracy:\t\t70.50 %\n",
      "Epoch 50 of 50 took 2.687s\n",
      "  training loss:\t\t0.581946\n",
      "  validation loss:\t\t0.599685\n",
      "  validation accuracy:\t\t69.50 %\n"
     ]
    }
   ],
   "source": [
    "num_epochs=50\n",
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(x_train, y_train, 2, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(x_val, y_val, 2, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test loss:\t\t\t0.696726\n",
      "  test accuracy:\t\t59.00 %\n"
     ]
    }
   ],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(x_test, y_test, 2, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
