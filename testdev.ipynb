{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test/Dev Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import chb\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Subject Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: organize this differently - this should just load the subject and\n",
    "#       data, so that we can loop through all the LOOs in main\n",
    "# Load data for subject\n",
    "subject = chb.CHBsubj()\n",
    "subject.load_meta('chb03')\n",
    "subject.load_data(exthd=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load and read training and test set images and labels.\n",
    "x_train, y_train, x_test, y_test = chb.leaveOneOut(subject, 1, 1000, 100)\n",
    "\n",
    "# We reserve the last 100 training examples for validation.\n",
    "x_train, x_val = x_train[:-100], x_train[-100:]\n",
    "y_train, y_val = y_train[:-100], y_train[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN from Scratch\n",
    "Well, not entirely. It's based on notes found [here](http://cs231n.github.io/neural-networks-1/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_size = (None, 1, 23, 256)\n",
    "output_size = 1 # Not sure if this is right! Maybe it should be 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne import layers\n",
    "from lasagne.nonlinearities import rectify, leaky_rectify, sigmoid\n",
    "from lasagne.objectives import binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard weight initialization for layers below is `W=lasagne.init.GlorotUniform()`, which we will not change (for now). It may be useful to change this to `lasagne.init.He()` or `lasagne.init.HeUniform()`, since I am using ReLU layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scratch_net(input_var):\n",
    "    \n",
    "    net = {}\n",
    "    net['data']  = layers.InputLayer(data_size, input_var=input_var)\n",
    "    net['conv1'] = layers.Conv2DLayer(net['data'],  num_filters=4, filter_size=(1,7), pad='same', \n",
    "                                      nonlinearity=rectify)\n",
    "    net['conv2'] = layers.Conv2DLayer(net['conv1'], num_filters=8, filter_size=(1,15), pad='same',\n",
    "                                      stride=(1,2), nonlinearity=rectify)\n",
    "    net['pool']  = layers.MaxPool2DLayer(net['conv2'], pool_size=(1,2))\n",
    "    net['fcl']   = layers.DenseLayer(net['pool'], num_units=256, nonlinearity=rectify)\n",
    "    net['out']   = layers.DenseLayer(net['fcl'], num_units=output_size, nonlinearity=sigmoid)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scratch_model(input_var, target_var, net):\n",
    "    \n",
    "    prediction = layers.get_output(net['out'])\n",
    "    loss = binary_crossentropy(prediction, target_var)\n",
    "    loss = lasagne.objectives.aggregate(loss)\n",
    "    \n",
    "    params = layers.get_all_params(net['out'], trainable=True)\n",
    "    updates = lasagne.updates.rmsprop(loss, params, learning_rate=1e-5)\n",
    "    \n",
    "    test_prediction = layers.get_output(net['out'], deterministic=True)\n",
    "    \n",
    "    test_loss = binary_crossentropy(test_prediction, target_var)\n",
    "    test_loss = lasagne.objectives.aggregate(test_loss)\n",
    "    test_acc  = T.mean(lasagne.objectives.binary_accuracy(test_prediction, target_var),dtype=theano.config.floatX)\n",
    "    \n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "    val_fn   = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "    \n",
    "    return train_fn, val_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scratch_train(train_fn, val_fn):\n",
    "    train_err_list = []\n",
    "    val_err_list = []\n",
    "    val_acc_list = []\n",
    "    \n",
    "    print('| epoch \\t| train loss\\t| val loss\\t| val acc\\t| time')\n",
    "    print('='*80)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        st = time.time()\n",
    "        batch_train_errs = []\n",
    "        for batch in iterate_minibatches(x_train, y_train, batch_size):\n",
    "            inputs, targets = batch\n",
    "            err = train_fn(inputs, targets)\n",
    "            batch_train_errs.append(err)   \n",
    "        epoch_train_err = np.mean(batch_train_errs)\n",
    "        train_err_list.append(epoch_train_err)\n",
    "        \n",
    "        batch_val_errs = []\n",
    "        batch_val_accs = []\n",
    "        for batch in iterate_minibatches(x_val, y_val, batch_size):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            batch_val_errs.append(err)\n",
    "            batch_val_accs.append(acc)\n",
    "        epoch_val_err = np.mean(batch_val_errs)\n",
    "        val_err_list.append(epoch_val_err)\n",
    "        epoch_val_acc = np.mean(batch_val_accs)\n",
    "        val_acc_list.append(epoch_val_acc)\n",
    "        \n",
    "        en = time.time()\n",
    "        print('| %d \\t\\t| %.6f\\t| %.6f\\t| %.2f%%\\t| %.2f s' \n",
    "              % (epoch+1, epoch_train_err, epoch_val_err, epoch_val_acc * 100, en-st))\n",
    "        #print('-'*80)\n",
    "    return train_err_list, val_err_list, val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scratch_test(val_fn):\n",
    "    \n",
    "    print('Test Results:')\n",
    "    print('='*80)\n",
    "\n",
    "    batch_err = []\n",
    "    batch_acc = []\n",
    "    for batch in iterate_minibatches(x_test, y_test, batch_size):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        batch_err.append(err)           \n",
    "        batch_acc.append(acc)\n",
    "            \n",
    "    test_err = np.mean(batch_err)\n",
    "    test_acc = np.mean(batch_acc)\n",
    "    \n",
    "    print('Test loss: %.6f \\t| Test accuracy: %.2f' % (test_err, test_acc * 100))\n",
    "    return test_err, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs=100\n",
    "batch_size=10\n",
    "\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "net = scratch_net(input_var)\n",
    "train_fn, val_fn = scratch_model(input_var, target_var, net)\n",
    "\n",
    "train_err, val_err, val_acc = scratch_train(train_fn, val_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(range(100), train_err, label='Training Error')\n",
    "ax1.plot(range(100), val_err, label='Validation Error')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(100), np.asarray(val_acc) * 100, color='g', label='Validation accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet Based On [This Site](https://github.com/luizgh/lasagne_basics/blob/master/comparing-optimization-algs.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_size=(None,1,23,256) # Batch size x Img Channels x Height (electrodes) x Width (samples)\n",
    "output_size=1             # Binary classification of seizure (1) vs non-seizure (0)\n",
    "\n",
    "def build_model():\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "    net = {}\n",
    "\n",
    "    #Input layer:\n",
    "    net['data'] = lasagne.layers.InputLayer(data_size, input_var=input_var)\n",
    "\n",
    "    #Convolution + Pooling\n",
    "    net['conv1'] = lasagne.layers.Conv2DLayer(net['data'], num_filters=6, filter_size=(1,16), stride=(1,2), nonlinearity=rectify)\n",
    "    net['pool1'] = lasagne.layers.Pool2DLayer(net['conv1'], pool_size=(1,2))\n",
    "\n",
    "    net['conv2'] = lasagne.layers.Conv2DLayer(net['pool1'], num_filters=10, filter_size=(1,32), stride=(1,2), nonlinearity=rectify)\n",
    "    net['pool2'] = lasagne.layers.Pool2DLayer(net['conv2'], pool_size=(1,2))\n",
    "\n",
    "\n",
    "    #Fully-connected + dropout\n",
    "    net['fc1'] = lasagne.layers.DenseLayer(net['pool2'], num_units=100)\n",
    "\n",
    "    #Output layer:\n",
    "    net['out'] = lasagne.layers.DenseLayer(net['fc1'], num_units=output_size, \n",
    "                                           nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    weight_decay = 1e-5\n",
    "\n",
    "    #Loss function: mean cross-entropy\n",
    "    prediction = lasagne.layers.get_output(net['out'])\n",
    "    loss = lasagne.objectives.binary_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    #Also add weight decay to the cost function\n",
    "    weightsl2 = lasagne.regularization.regularize_network_params(net['out'], lasagne.regularization.l2)\n",
    "    loss += weight_decay * weightsl2\n",
    "\n",
    "    #Get the update rule\n",
    "    params = lasagne.layers.get_all_params(net['out'], trainable=True)\n",
    "    updates = lasagne.updates.sgd(loss, params, learning_rate=1e-2)\n",
    "\n",
    "    test_prediction = lasagne.layers.get_output(net['out'], deterministic=True)\n",
    "    test_loss = lasagne.objectives.binary_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, name='train')\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc], name='validation')\n",
    "    get_preds = theano.function([input_var], test_prediction, name='get_preds')\n",
    "\n",
    "    return (train_fn, val_fn, get_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size=10\n",
    "\n",
    "#Run the training function per mini-batches\n",
    "n_examples = x_train.shape[0]\n",
    "n_batches = int(n_examples / batch_size)\n",
    "\n",
    "def train(train_fn):\n",
    "    cost_history = []\n",
    "    for epoch in range(epochs):\n",
    "        st = time.time()\n",
    "        batch_cost_history = []\n",
    "        for batch in range(n_batches):\n",
    "            x_batch = x_train[batch*batch_size: (batch+1) * batch_size]\n",
    "            y_batch = y_train[batch*batch_size: (batch+1) * batch_size]\n",
    "\n",
    "            this_cost = train_fn(x_batch, y_batch) # This is where the model gets updated\n",
    "\n",
    "            batch_cost_history.append(this_cost)\n",
    "        epoch_cost = np.mean(batch_cost_history)\n",
    "        cost_history.append(epoch_cost)\n",
    "        en = time.time()\n",
    "        print('Epoch %d/%d, train error: %f. Elapsed time: %.2f seconds' % (epoch+1, epochs, epoch_cost, en-st))\n",
    "    return cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd_functions = build_model()\n",
    "for key, value in network.items():\n",
    "    print('%s: %s' % (key, layers.get_output_shape(value)))\n",
    "print (\"Training with SGD\")\n",
    "sgd_cost_history = train(sgd_functions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet Based On mnist.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ##################### Build the neural network model #######################\n",
    "def build_cnn(input_var=None):\n",
    "    data_size = (None,1,23,256)\n",
    "    output_size = 1\n",
    "\n",
    "    network = {}\n",
    "    \n",
    "    network['data'] = layers.InputLayer(shape=data_size, input_var=input_var)\n",
    "\n",
    "    network['conv1'] = layers.Conv2DLayer(network['data'], num_filters=8, filter_size=(1, 15),\n",
    "                                        stride=(1,1), nonlinearity=rectify,pad='same')\n",
    "\n",
    "    network['pool1'] = layers.MaxPool2DLayer(network['conv1'], pool_size=(1,2))\n",
    "\n",
    "    network['conv2'] = layers.Conv2DLayer(network['pool1'], num_filters=8, filter_size=(1, 15), \n",
    "                                        stride=(1,2), nonlinearity=rectify)\n",
    "    \n",
    "    network['dense1'] = layers.DenseLayer(network['conv2'],num_units=256, nonlinearity=rectify)\n",
    "\n",
    "    network['out'] = layers.DenseLayer(network['dense1'], num_units=output_size, nonlinearity=rectify)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "# Notice that this function returns only mini-batches of size `batchsize`.\n",
    "# If the size of the data is not a multiple of `batchsize`, it will not\n",
    "# return the last (remaining) mini-batch.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "input_var1 = T.tensor4('inputs')\n",
    "target_var1 = T.ivector('targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create neural network model (depending on first command line parameter)\n",
    "print(\"Building model and compiling functions...\")\n",
    "network = build_cnn(input_var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a loss expression for training, i.e., a scalar objective we want\n",
    "# to minimize (for our 2-class problem, it is the cross-entropy loss):\n",
    "prediction1 = layers.get_output(network['out'])\n",
    "\n",
    "loss1 = binary_crossentropy(prediction1, target_var1)\n",
    "loss1 = loss1.mean()\n",
    "\n",
    "# Update expressions for training\n",
    "params1 = layers.get_all_params(network['out'], trainable=True)\n",
    "updates1 = lasagne.updates.rmsprop(loss1, params1, learning_rate=0.001)\n",
    "\n",
    "# Create a loss expression for validation/testing.\n",
    "test_prediction1 = layers.get_output(network['out'], deterministic=True)\n",
    "\n",
    "test_loss1 = binary_crossentropy(test_prediction1,target_var1)\n",
    "test_loss1 = test_loss1.mean()\n",
    "\n",
    "test_acc1 = T.mean(T.eq(T.argmax(test_prediction1, axis=1), target_var1),dtype=theano.config.floatX)\n",
    "\n",
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn1 = theano.function([input_var1, target_var1], loss1, updates=updates1)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn1 = theano.function([input_var1, target_var1], [test_loss1, test_acc1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, value in network.items():\n",
    "    print('%s: %s' % (key, layers.get_output_shape(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(x_train, y_train, 10):\n",
    "        inputs, targets = batch\n",
    "        #print(inputs.shape, targets.shape)\n",
    "        train_err += train_fn1(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(x_val, y_val, 10):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn1(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training error:\\t\\t{:.6f}\".format(train_err)) #\n",
    "    print(\"  train batches:\\t\\t{:d}\".format(train_batches)) #\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation error:\\t\\t{:.6f}\".format(val_err)) #\n",
    "    print(\"  validation batches:\\t\\t{:d}\".format(val_batches)) #\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(x_test, y_test, 2, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
